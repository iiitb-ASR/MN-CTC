{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from pytorch_end2end import CTCDecoder\n",
    "from jiwer import wer\n",
    "from dataloader.ctc_support_set_loader import SupportDataSet, collate_wrapper, stackup\n",
    "from dataloader.ctc_batch_loader import QueryDataSet, batch_stackup, pad_batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import Configuration as config\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "seed=config.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn embedding - encoder_g  (SS)\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv_block(in_channels: int, out_channels: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "def few_shot_cnn_encoder(num_input_channels=1, outchnls=64) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        conv_block(num_input_channels, outchnls),\n",
    "        conv_block(outchnls, outchnls),\n",
    "        conv_block(outchnls, outchnls),\n",
    "        Flatten(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm embedding - encoder_f (Batch utterances)\n",
    "class few_shot_lstm_encoder(nn.Module):\n",
    "    def __init__(self, lstm_inp_dim):\n",
    "        super(few_shot_lstm_encoder, self).__init__()\n",
    "        \n",
    "        self.count = 0\n",
    "        \n",
    "        # LSTM parameters\n",
    "        self.lstm_inp_dim = lstm_inp_dim  #39\n",
    "        self.lstm_hidden_dim = 128\n",
    "        self.lstm_layers = 2\n",
    "        self.batch_norm = True\n",
    "                \n",
    "        self.bilstm = nn.LSTM(input_size=self.lstm_inp_dim, hidden_size=self.lstm_hidden_dim, num_layers=self.lstm_layers,\n",
    "                              bidirectional=True,batch_first=True)\n",
    "        \n",
    "     \n",
    "    def forward(self, input):\n",
    "        ys, _ = self.bilstm(input)  #1024\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, n, k, q, num_input_channels, lstm_input_size,use_cuda=False):\n",
    " \n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.q = q\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.lstm_input_size = lstm_input_size\n",
    "        self.encoder_g = few_shot_cnn_encoder(self.num_input_channels)    # cnn encoder for SS , emb_size = 256\n",
    "        self.encoder_f = few_shot_lstm_encoder(self.lstm_input_size)     # LSTM encoder for batch , emb_size = 256\n",
    "        \n",
    "        isCudaAvailable = torch.cuda.is_available()\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        if isCudaAvailable & self.use_cuda:\n",
    "            self.device=torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device=torch.device('cpu')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceNetwork(nn.Module):\n",
    "    def __init__(self,n,k,q,bshot):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "        self.n=n\n",
    "        self.k=k\n",
    "        self.q=q\n",
    "        self.bshot=bshot\n",
    "\n",
    "    def forward(self, support_set, query_set):\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        \n",
    "        f = self.bshot  # total no:of blanks in support set\n",
    "        sum_support = torch.sum(torch.pow(support_set, 2), 2)\n",
    "        support_magnitude = sum_support.clamp(eps, float(\"inf\")).rsqrt()\n",
    "        dot_product = query_set.matmul(support_set.permute(0,2,1))\n",
    "             \n",
    "        cosine_similarity = dot_product * (support_magnitude.unsqueeze(1))  # for ctc\n",
    "        cosine_similarity  = torch.cat( [cosine_similarity[:,:, :f].topk(self.n, dim=2).values, cosine_similarity [:,:, f:]], dim=2) # take top n blanks    \n",
    "        return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class episodeBuilder:\n",
    "    def __init__(self, model, n_shot, k_way, b_shot, q_queries,    # changes for handling more blank samples \n",
    "                 batch_size,\n",
    "                 data_train=None,data_val=None,data_test=None,batch_test=None):\n",
    "        \n",
    "        self.optimiser = Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.label_dict = config.label_dict\n",
    "        self.ctc_labels = config.ctc_labels\n",
    "        \n",
    "        self.decoder = CTCDecoder(blank_idx=0, beam_width=100, time_major=False,after_logsoftmax=True,labels=self.ctc_labels)\n",
    "         \n",
    "        \n",
    "        self.model=model\n",
    "    \n",
    "        self.n_shot=n_shot\n",
    "        self.b_shot=b_shot  # changes for handling more blank samples\n",
    "        self.k_way=k_way\n",
    "        self.q_queries=q_queries\n",
    "        self.batch_size=batch_size\n",
    "        self.batch_test = batch_test\n",
    "        self.dn = DistanceNetwork(n_shot,k_way,q_queries,b_shot)\n",
    "\n",
    "        self.data_test=data_test\n",
    "        \n",
    "        self.device=self.model.device\n",
    "        \n",
    "    def each_episode_ctc(self, X,y,batch_query, trainflag=True):\n",
    "       \n",
    "        self.train = trainflag\n",
    "        \n",
    "        if self.train:\n",
    "            # Zero gradients\n",
    "            self.model.train()\n",
    "            self.optimiser.zero_grad()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        self.batch_size=X.size(0)\n",
    "        \n",
    "        inputs, input_sizes, targets, target_sizes, utt = batch_query\n",
    "        inputs = inputs.to(self.device)\n",
    "               \n",
    "        inputs = torch.squeeze(inputs,2)\n",
    "        inputs = inputs.reshape(inputs.shape[0],inputs.shape[1], -1)  #(batch_size,frames,inp_dim)\n",
    "        \n",
    "        # Embed all query utterances using LSTM ( to use the sequence information - f)\n",
    "        embeddings_X_ctc = self.model.encoder_f(inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Embed SS samples using CNN embedding (g)\n",
    "        encoded_items_X = []\n",
    "        for i in np.arange(self.batch_size):\n",
    "            ssi_X = X[i]\n",
    "            gen_encode_X = self.model.encoder_g(ssi_X)\n",
    "            encoded_items_X.append(gen_encode_X)\n",
    "        embeddings_X = torch.stack(encoded_items_X)\n",
    "        \n",
    "        support = embeddings_X\n",
    "        queries = embeddings_X_ctc\n",
    "                \n",
    "        similarities = self.dn(support_set=support, query_set=queries)\n",
    "        softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "        attention = self.matching_net_predictions(similarities)\n",
    "        ypreds = softmax(attention)   # [batch_size , len, num_classes]\n",
    "        \n",
    "        ypreds = ypreds.to(self.device, dtype=torch.float32)  \n",
    "        ypreds_sizes = torch.zeros(self.batch_size)\n",
    "        ypreds_sizes[0] = ypreds.shape[1]\n",
    "        ypreds_sizes = ypreds_sizes.long().to(self.device)\n",
    "        values, indices = torch.max(ypreds, dim=-1)\n",
    "        decoded_targets, decoded_targets_lengths, decoded_sentences = self.decoder.decode(ypreds,ypreds_sizes)\n",
    "        target_label_indices = torch.squeeze(targets,0).tolist()\n",
    "        target_label = [self.label_dict[ele] for ele in target_label_indices]\n",
    "        pred_index = decoded_targets.view(decoded_targets.shape[1]).tolist()\n",
    "        \n",
    "\n",
    "        pred_label = [self.label_dict[ele] for ele in pred_index]\n",
    "        per = wer(target_label,pred_label)\n",
    "        \n",
    "        return per\n",
    "\n",
    "    def matching_net_predictions(self, attention):\n",
    "        \"\"\"Calculates Matching Network predictions based on equation (1) of the paper.\n",
    "        \"\"\"\n",
    "        q=self.q_queries\n",
    "        k=self.k_way\n",
    "        n=self.n_shot\n",
    "        \n",
    "       \n",
    "        y_preds=[]\n",
    "        for eachbatch in range(attention.size(0)):\n",
    "            # Create one hot label vector for the support set\n",
    "            y_onehot = torch.zeros(k * n, k)\n",
    "            \n",
    "            ys = self.create_nshot_task_label(k, n).unsqueeze(-1)       \n",
    "            y_onehot = y_onehot.scatter(1, ys, 1)\n",
    "        \n",
    "            y_pred = torch.mm(attention[eachbatch], y_onehot.to(self.device, dtype=torch.float32))\n",
    "            y_preds.append(y_pred)\n",
    "            \n",
    "        y_preds=torch.stack(y_preds)\n",
    "\n",
    "        return y_preds\n",
    "    \n",
    "    def create_nshot_task_label(self, k, n):\n",
    "        return torch.arange(0, k, 1 / n).long()    \n",
    "    \n",
    "    def run_training_epoch(self, total_train_batches):\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        with tqdm(total=total_train_batches, desc='train batches:', leave=False) as pbar:\n",
    "            for i in range(total_train_batches):\n",
    "                data = next(iter(self.data_train))\n",
    "                X,y,ylabels=data\n",
    "                X=X.to(self.device, dtype=torch.float32)\n",
    "                y=y.to(self.device, dtype=torch.long)\n",
    "                loss, acc, _ = self.each_episode(X, y, trainflag=True)\n",
    "                \n",
    "                #raise Exception(\"Forced break...testing\")\n",
    "                \n",
    "                loss.backward()\n",
    "                clip_grad_norm_(self.model.parameters(), 1)\n",
    "                self.optimiser.step()\n",
    "                pbar.update(1)\n",
    "                total_loss += loss.data\n",
    "                total_accuracy += acc.data\n",
    "\n",
    "            total_loss = total_loss / total_train_batches\n",
    "            total_accuracy = total_accuracy / total_train_batches\n",
    "            return total_loss, total_accuracy\n",
    "\n",
    "        \n",
    "    def run_test_epoch(self, total_test_batches):\n",
    "        total_per = 0.0\n",
    "        with tqdm(total=total_test_batches, desc='test batches:', leave=False) as pbar:\n",
    "            pred_list=[]\n",
    "            for i,  batch_query in enumerate(self.batch_test):   # to iterate through all utterances in an epoch\n",
    "                support_data = next(iter(self.data_test))\n",
    "                X,y,ylabels=support_data\n",
    "                X=X.to(self.device, dtype=torch.float32)\n",
    "                y=y.to(self.device, dtype=torch.long)\n",
    "                per = self.each_episode_ctc(X, y, batch_query, trainflag=False)\n",
    "                total_per += per\n",
    "                print(\"test epoch  \", i,  \"PER => \", per, \"  and avg PER  \",total_per/(i+1))\n",
    "                pbar.update(1)\n",
    "                \n",
    "        total_per = total_per / total_test_batches\n",
    "        return total_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_train, k_train, q_train, fce=False, use_cuda=False):\n",
    "    num_input_channels=1 \n",
    "    lstm_input_size = 39 # encoder output   ## check\n",
    "\n",
    "    model = MatchingNetwork(n_train, k_train, q_train, num_input_channels, lstm_input_size,      # include lstm size\n",
    "                            use_cuda=use_cuda)\n",
    "\n",
    "    model=model.to(model.device, dtype=torch.float32)\n",
    "    return model\n",
    "\n",
    "def load_model(fpath, n_train, k_train, q_train, use_cuda=False, eval_flag=True):\n",
    "    model = build_model(n_train, k_train, q_train, use_cuda=use_cuda)\n",
    "    model.load_state_dict(torch.load(fpath,map_location=model.device))\n",
    "    model = model.to(model.device, dtype=torch.float32)\n",
    "    if eval_flag:\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=1;#q=1 is supported as of now\n",
    "batch_size=1 # batch_size=1 is recommended as of now\n",
    "\n",
    "n = config.N\n",
    "k = config.K\n",
    "bshot = config.blanks_inference\n",
    "\n",
    "inference_model=load_model(config.saved_model_for_inference, n, k, q, use_cuda=config.use_cuda, eval_flag=True)\n",
    "\n",
    "\n",
    "test_dataset=SupportDataSet(supportdatafile=config.devSupportSet, kway=k, nshot=n, bshot=bshot, nqueries=q, \n",
    "                            phonemes=config.ALLPHONEMES, index_dict=config.index_dict, label_dict=config.label_dict,\n",
    "                           transform=transforms.Compose([ stackup()]))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_wrapper)\n",
    "\n",
    " \n",
    "\n",
    "batch_dataset=QueryDataSet(querysetfile=config.testQuerySet,transform=transforms.Compose([ batch_stackup(config.index_dict)]))                              \n",
    "batch_loader = DataLoader(batch_dataset, batch_size=batch_size, collate_fn=pad_batch)\n",
    "\n",
    "episode1 = episodeBuilder(inference_model, n, k, bshot, q, batch_size,\n",
    "                          data_test=test_dataloader,\n",
    "                          batch_test=batch_loader)\n",
    "\n",
    "total_test_batches = batch_loader.__len__()\n",
    "per = episode1.run_test_epoch(total_test_batches)\n",
    "print(\"Per of network : \", per*100)  # blanks 10 shots per phn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
