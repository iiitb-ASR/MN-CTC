{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MN+LSTM+CTC training using SS with untagged blanks (LSTM - 2 layer, inp_dim = 429 and embedding dim = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from pytorch_end2end import CTCDecoder\n",
    "from jiwer import wer\n",
    "from dataloader.ctc_support_set_loader import SupportDataSet, collate_wrapper, stackup\n",
    "from dataloader.ctc_batch_loader import QueryDataSet, batch_stackup, pad_batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import Configuration as config\n",
    "from tqdm.notebook import tqdm\n",
    "import editdistance as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "seed=25\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn embedding - encoder_g  (SS)\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def conv_block(in_channels: int, out_channels: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "def few_shot_cnn_encoder(num_input_channels=1, outchnls=64) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        conv_block(num_input_channels, outchnls),\n",
    "        conv_block(outchnls, outchnls),\n",
    "        conv_block(outchnls, outchnls),\n",
    "        Flatten(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm embedding - encoder_f (Batch utterances)\n",
    "class few_shot_lstm_encoder(nn.Module):\n",
    "    def __init__(self, lstm_inp_dim):\n",
    "        super(few_shot_lstm_encoder, self).__init__()\n",
    "        \n",
    "        self.count = 0\n",
    "        \n",
    "        # LSTM parameters\n",
    "        self.lstm_inp_dim = lstm_inp_dim  #39\n",
    "        self.lstm_hidden_dim = 128\n",
    "        self.lstm_layers = 2\n",
    "        self.batch_norm = True\n",
    "                \n",
    "        self.bilstm = nn.LSTM(input_size=self.lstm_inp_dim, hidden_size=self.lstm_hidden_dim, num_layers=self.lstm_layers,\n",
    "                              bidirectional=True,batch_first=True)\n",
    "        \n",
    "     \n",
    "    def forward(self, input):\n",
    "        ys, _ = self.bilstm(input)  #1024\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, n, k, q, num_input_channels, lstm_input_size,use_cuda=False):\n",
    "\n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.q = q\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.lstm_input_size = lstm_input_size\n",
    "        self.encoder_g = few_shot_cnn_encoder(self.num_input_channels)    # cnn encoder for SS , emb_size = 256\n",
    "        self.encoder_f = few_shot_lstm_encoder(self.lstm_input_size)     # LSTM encoder for batch , emb_size = 256\n",
    "        \n",
    "        isCudaAvailable = torch.cuda.is_available()\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        if isCudaAvailable & self.use_cuda:\n",
    "            self.device=torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device=torch.device('cpu')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceNetwork(nn.Module):\n",
    "    def __init__(self,n,k,q,bshot):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "        self.n=n\n",
    "        self.k=k\n",
    "        self.q=q\n",
    "        self.bshot=bshot\n",
    "\n",
    "    def forward(self, support_set, query_set):\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        \n",
    "        f = self.bshot  # total no:of blanks in support set\n",
    "        sum_support = torch.sum(torch.pow(support_set, 2), 2)\n",
    "        support_magnitude = sum_support.clamp(eps, float(\"inf\")).rsqrt()\n",
    "        dot_product = query_set.matmul(support_set.permute(0,2,1))\n",
    "             \n",
    "        cosine_similarity = dot_product * (support_magnitude.unsqueeze(1))  # for ctc\n",
    "        cosine_similarity  = torch.cat( [cosine_similarity[:,:, :f].topk(self.n, dim=2).values, cosine_similarity [:,:, f:]], dim=2) # take top n blanks    \n",
    "        return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class episodeBuilder:\n",
    "    def __init__(self, model, n_shot, k_way, b_shot, q_queries=1,    # changes for handling more blank samples\n",
    "                 batch_size=1,\n",
    "                 data_train=None,batch_train=None,batch_val=None,batch_test=None):\n",
    "        \n",
    "        self.ctc_loss = nn.CTCLoss(reduction='sum')\n",
    "        self.optimiser = Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.label_dict = config.label_dict\n",
    "        self.ctc_labels = config.ctc_labels\n",
    "        \n",
    "        self.decoder = CTCDecoder(blank_idx=0, beam_width=100, time_major=False,after_logsoftmax=True,labels=self.ctc_labels)\n",
    "         \n",
    "        \n",
    "        self.model=model\n",
    "    \n",
    "        self.n_shot=n_shot\n",
    "        self.b_shot=b_shot  # changes for handling more blank samples\n",
    "        self.k_way=k_way\n",
    "        self.q_queries=q_queries\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.dn = DistanceNetwork(n_shot,k_way,q_queries,b_shot)\n",
    "\n",
    "        self.data_train = data_train\n",
    "        self.batch_train = batch_train\n",
    "        self.batch_val = batch_val\n",
    "        self.batch_test = batch_test\n",
    "        \n",
    "        self.device=self.model.device\n",
    "    \n",
    "    def compute_wer(self, index, input_sizes, targets, target_sizes):\n",
    "        batch_errs = 0\n",
    "        batch_tokens = 0\n",
    "        for i in range(len(index)):\n",
    "            label = targets[i][:target_sizes[i]]\n",
    "            pred = []\n",
    "            for j in range(len(index[i][:input_sizes[i]])):\n",
    "                if index[i][j] == 0:\n",
    "                    continue\n",
    "                if j == 0:\n",
    "                    pred.append(index[i][j])\n",
    "                if j > 0 and index[i][j] != index[i][j-1]:\n",
    "                    pred.append(index[i][j])\n",
    "            batch_errs += ed.eval(label, pred)\n",
    "            batch_tokens += len(label)\n",
    "        return batch_errs, batch_tokens\n",
    "\n",
    "    def each_episode_ctc(self, X,y,batch_query, trainflag=True):\n",
    "    \n",
    "        self.train = trainflag\n",
    "        if self.train:\n",
    "            # Zero gradients\n",
    "            self.model.train()\n",
    "            self.optimiser.zero_grad()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        self.batch_size=X.size(0)\n",
    "        \n",
    "        inputs, input_sizes, targets, target_sizes, utt = batch_query\n",
    "        inputs = inputs.to(self.device)\n",
    "        input_sizes = input_sizes.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        target_sizes = target_sizes.to(self.device)\n",
    "        \n",
    "        inputs = torch.squeeze(inputs,2)\n",
    "        inputs = inputs.reshape(inputs.shape[0],inputs.shape[1], -1)  #(batch_size,frames,inp_dim)\n",
    "        \n",
    "        # Embed all batch utterances using LSTM ( to use the sequence information - f)\n",
    "        embeddings_X_ctc = self.model.encoder_f(inputs)\n",
    "        \n",
    "        # Embed SS samples using CNN embedding (g)\n",
    "        encoded_items_X = []\n",
    "        for i in np.arange(self.batch_size):\n",
    "            ssi_X = X[i]\n",
    "            gen_encode_X = self.model.encoder_g(ssi_X)\n",
    "            encoded_items_X.append(gen_encode_X)\n",
    "        embeddings_X = torch.stack(encoded_items_X)\n",
    "\n",
    "        support = embeddings_X\n",
    "        queries = embeddings_X_ctc\n",
    "               \n",
    "        similarities = self.dn(support_set=support, query_set=queries)\n",
    "        softmax = nn.LogSoftmax(dim=2)        \n",
    "        attention = self.matching_net_predictions(similarities)\n",
    "        ypreds = softmax(attention)\n",
    "        \n",
    "        ypreds = ypreds.transpose(0,1)  # [len, batch_size , num_classes]\n",
    "        ypreds = ypreds.to(self.device, dtype=torch.float32)   \n",
    "        \n",
    "        out_len, b_size, _ = ypreds.size()\n",
    "        input_sizes = (input_sizes * out_len).long()\n",
    "        \n",
    "        loss = self.ctc_loss(ypreds, targets, input_sizes, target_sizes)\n",
    "         \n",
    "        values, indices = torch.max(ypreds, dim=-1)\n",
    "        \n",
    "        \n",
    "        batch_errs, batch_tokens = self.compute_wer(indices.transpose(0,1).cpu().numpy(), input_sizes.cpu().numpy(), \n",
    "                                                    targets.cpu().numpy(), target_sizes.cpu().numpy())\n",
    "        \n",
    "        error = batch_errs/batch_tokens\n",
    "        return loss, error\n",
    "    \n",
    "    def each_episode_ctc_test(self, X,y,batch_query, trainflag=False):\n",
    "        \n",
    "        self.train = trainflag\n",
    "        \n",
    "        if self.train:\n",
    "            # Zero gradients\n",
    "            self.model.train()\n",
    "            self.optimiser.zero_grad()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        self.batch_size=X.size(0)\n",
    "        \n",
    "        inputs, input_sizes, targets, target_sizes, utt = batch_query\n",
    "        inputs = inputs.to(self.device)\n",
    "        \n",
    "        \n",
    "        inputs = torch.squeeze(inputs,2)\n",
    "        inputs = inputs.reshape(inputs.shape[0],inputs.shape[1], -1)  #(batch_size,frames,inp_dim)\n",
    "        \n",
    "        # Embed all query utterances using LSTM ( to use the sequence information - f)\n",
    "        embeddings_X_ctc = self.model.encoder_f(inputs)\n",
    "        \n",
    "        # Embed SS samples using CNN embedding (g)\n",
    "        encoded_items_X = []\n",
    "        for i in np.arange(self.batch_size):\n",
    "            ssi_X = X[i]\n",
    "            gen_encode_X = self.model.encoder_g(ssi_X)\n",
    "            encoded_items_X.append(gen_encode_X)\n",
    "        embeddings_X = torch.stack(encoded_items_X)\n",
    "\n",
    "        support = embeddings_X\n",
    "        queries = embeddings_X_ctc\n",
    "    \n",
    "        \n",
    "        similarities = self.dn(support_set=support, query_set=queries)\n",
    "        softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        attention = self.matching_net_predictions(similarities)\n",
    "        ypreds = softmax(attention)   # [batch_size , len, num_classes]\n",
    "        \n",
    "        ypreds = ypreds.to(self.device, dtype=torch.float32)  \n",
    "        ypreds_sizes = torch.zeros(self.batch_size)\n",
    "        ypreds_sizes[0] = ypreds.shape[1]\n",
    "        ypreds_sizes = ypreds_sizes.long().to(self.device)\n",
    "        \n",
    "        values, indices = torch.max(ypreds, dim=-1)\n",
    "            \n",
    "        decoded_targets, decoded_targets_lengths, decoded_sentences = self.decoder.decode(ypreds,ypreds_sizes)\n",
    "        \n",
    "        target_label_indices = torch.squeeze(targets,0).tolist()\n",
    "        target_label = [self.label_dict[ele] for ele in target_label_indices]\n",
    "        pred_index = decoded_targets.view(decoded_targets.shape[1]).tolist()\n",
    "\n",
    "        pred_label = [self.label_dict[ele] for ele in pred_index]\n",
    "        per = wer(target_label,pred_label)\n",
    "        \n",
    "        return per\n",
    "     \n",
    "    def matching_net_predictions(self, attention):\n",
    "        \"\"\"Calculates Matching Network predictions based on equation (1) of the paper.\n",
    "        \"\"\"\n",
    "        q=self.q_queries\n",
    "        k=self.k_way\n",
    "        n=self.n_shot\n",
    "        \n",
    "        y_preds=[]\n",
    "        for eachbatch in range(attention.size(0)):\n",
    "            # Create one hot label vector for the support set\n",
    "            y_onehot = torch.zeros(k * n, k)\n",
    "            ys = self.create_nshot_task_label(k, n).unsqueeze(-1)       \n",
    "            y_onehot = y_onehot.scatter(1, ys, 1)\n",
    "            \n",
    "            y_pred = torch.mm(attention[eachbatch], y_onehot.to(self.device, dtype=torch.float32))\n",
    "            y_preds.append(y_pred)\n",
    "            \n",
    "        y_preds=torch.stack(y_preds)\n",
    "\n",
    "        return y_preds\n",
    "\n",
    "    def create_nshot_task_label(self, k, n):\n",
    "        return torch.arange(0, k, 1 / n).long()    \n",
    "    \n",
    "    def run_training_epoch(self, total_train_batches):\n",
    "        total_loss = 0.0\n",
    "        total_error = 0.0\n",
    "        total_frame_acc = 0.0\n",
    "        \n",
    "        with tqdm(total=total_train_batches, desc='train', leave=False) as pbar1: \n",
    "            for i,  batch_query in enumerate(self.batch_train):  # to iterate through all utterances in an epoch\n",
    "                support_data = next(iter(self.data_train))\n",
    "                X,y,ylabels=support_data\n",
    "                X=X.to(self.device, dtype=torch.float32)\n",
    "                y=y.to(self.device, dtype=torch.long)\n",
    "                               \n",
    "                loss, err = self.each_episode_ctc(X, y, batch_query, trainflag=True)\n",
    "                \n",
    "                total_loss += loss\n",
    "                total_error += err\n",
    "               \n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "        \n",
    "                pbar1.update(1)\n",
    "        \n",
    "        total_loss = total_loss / total_train_batches\n",
    "        total_error = total_error / total_train_batches\n",
    "        \n",
    "        return total_loss, total_error\n",
    "        \n",
    "    def run_val_epoch(self, total_val_batches):\n",
    "        total_loss = 0.0\n",
    "        total_error = 0.0\n",
    "        total_frame_acc = 0.0\n",
    "               \n",
    "        with tqdm(total=total_val_batches, desc='val', leave=False) as pbar1:\n",
    "            for i,  batch_query in enumerate(self.batch_val):  # to iterate through all utterances in an epoch\n",
    "                support_data = next(iter(self.data_train))\n",
    "                X,y,ylabels=support_data\n",
    "                X=X.to(self.device, dtype=torch.float32)\n",
    "                y=y.to(self.device, dtype=torch.long)\n",
    "                               \n",
    "                loss, err = self.each_episode_ctc(X, y, batch_query, trainflag=False)\n",
    "\n",
    "                total_loss += loss.data\n",
    "                total_error += err\n",
    "                \n",
    "                pbar1.update(1)\n",
    "\n",
    "        total_loss = total_loss / total_val_batches\n",
    "        total_error = total_error / total_val_batches\n",
    "        return total_loss, total_error#, total_frame_acc\n",
    "\n",
    "    def run_test_epoch(self, total_test_batches):\n",
    "        total_per = 0.0\n",
    "        \n",
    "        with tqdm(total=total_test_batches, desc='test batches:', leave=False) as pbar:\n",
    "            pred_list=[]\n",
    "            for i,  batch_query in enumerate(self.batch_test):   # to iterate through all utterances in an epoch\n",
    "                support_data = next(iter(self.data_train))\n",
    "                X,y,ylabels=support_data\n",
    "                X=X.to(self.device, dtype=torch.float32)\n",
    "                y=y.to(self.device, dtype=torch.long)\n",
    "\n",
    "                per = self.each_episode_ctc_test(X, y, batch_query, trainflag=False)\n",
    "                total_per += per\n",
    "                pbar.update(1)\n",
    "                \n",
    "            total_per = total_per / total_test_batches\n",
    "            return(total_per)\n",
    "    \n",
    "    def save_model(self, tepochs, fpath):\n",
    "        fpath=fpath[:-4]+'_'+str(self.k_way)+'_'+str(self.n_shot)+ '_'+str(self.b_shot)+ '_'+str(tepochs)+fpath[-4:]\n",
    "        torch.save(self.model.state_dict(), fpath)\n",
    "        return fpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_train, k_train, q_train, fce=False, use_cuda=False):\n",
    "    num_input_channels=1 \n",
    "    lstm_input_size = 39 # encoder output   ## check\n",
    "\n",
    "    model = MatchingNetwork(n_train, k_train, q_train, num_input_channels, lstm_input_size,      # include lstm size\n",
    "                            use_cuda=use_cuda)\n",
    "\n",
    "    model=model.to(model.device, dtype=torch.float32)\n",
    "    return model\n",
    "\n",
    "def load_model(fpath, n_train, k_train, q_train, use_cuda=False, eval_flag=True):\n",
    "    model = build_model(n_train, k_train, q_train, use_cuda=use_cuda)\n",
    "    model.load_state_dict(torch.load(fpath,map_location=model.device))\n",
    "    model = model.to(model.device, dtype=torch.float32)\n",
    "    if eval_flag:\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K way N Shot.. \n",
    "n = config.Q\n",
    "k = config.P\n",
    "\n",
    "bshot=config.blanks_train;\n",
    "q=1; #As of now q=1 only supported\n",
    "batch_size=1 #As of now batch_size=1 supported\n",
    "\n",
    "\n",
    "train_dataset=SupportDataSet(supportdatafile=config.trainSupportSet, kway=k, nshot=n, bshot=bshot, nqueries=q, phonemes=config.ALLPHONEMES, index_dict=config.index_dict, label_dict=config.label_dict,\n",
    "                           transform=transforms.Compose([ stackup()]))     # changes for handling more blank samples\n",
    "                          \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_wrapper)\n",
    "\n",
    "\n",
    "train_batch_dataset=QueryDataSet(querysetfile=config.trainQuerySet,transform=transforms.Compose([ batch_stackup(config.index_dict)]))                              \n",
    "train_batch_loader = DataLoader(train_batch_dataset, batch_size=batch_size, collate_fn=pad_batch) \n",
    "\n",
    "\n",
    "\n",
    "val_batch_dataset=QueryDataSet(querysetfile=config.devQuerySet,transform=transforms.Compose([ batch_stackup(config.index_dict)]))                              \n",
    "val_batch_loader = DataLoader(val_batch_dataset, batch_size=batch_size, collate_fn=pad_batch)\n",
    "\n",
    "\n",
    "test_batch_dataset=QueryDataSet(querysetfile=config.testQuerySet,transform=transforms.Compose([ batch_stackup(config.index_dict)]))                              \n",
    "test_batch_loader = DataLoader(test_batch_dataset, batch_size=batch_size, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_epochs = config.prev_model_epochs\n",
    "\n",
    "if old_epochs!=0:\n",
    "    model = load_model(config.prev_model_path, n, k, q, use_cuda=True, eval_flag=False)\n",
    "else:\n",
    "    model = build_model(n, k, q, use_cuda=config.use_cuda)\n",
    "\n",
    "model = model.to(model.device, dtype=torch.float32)\n",
    "        \n",
    "epochs = config.epochs\n",
    "\n",
    "total_train_batches = train_batch_loader.__len__() \n",
    "total_val_batches = val_batch_loader.__len__()\n",
    "\n",
    "episode = episodeBuilder(model, n, k, bshot, q, batch_size, \n",
    "                          data_train=train_dataloader,\n",
    "                          batch_train=train_batch_loader,\n",
    "                          batch_val=val_batch_loader,\n",
    "                          batch_test=test_batch_loader)\n",
    "\n",
    "logdir=config.model_store_path\n",
    "\n",
    "try:\n",
    "    os.stat(logdir)\n",
    "except:\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MN_CTC Training started...')\n",
    "loss, acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "\n",
    "start_time = datetime.now()\n",
    "with tqdm(total=epochs, desc='epochs', leave=False) as pbar:\n",
    "    for e in range(epochs):\n",
    "        epoch_start_time = datetime.now()\n",
    "        \n",
    "        total_c_loss, total_error = episode.run_training_epoch(total_train_batches)\n",
    "        loss.append(total_c_loss); acc.append(total_error)\n",
    "        total_val_c_loss, total_val_error = episode.run_val_epoch(total_val_batches)\n",
    "        val_loss.append(total_val_c_loss); val_acc.append(total_val_error)\n",
    "\n",
    "        print(\"Epoch {}: train: [loss-{:.6f} error-{:.6f} ], val: [loss-{:.6f} error-{:.6f}]\".                 \n",
    "              format(e+old_epochs, total_c_loss.item(), total_error,                 \n",
    "                     total_val_c_loss.item(), total_val_error))\n",
    "        pbar.update(1)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        print('Epoch Training time:', end_time-epoch_start_time)\n",
    "        # save model\n",
    "        modelpath = episode.save_model(e+old_epochs,fpath= logdir + '/model.pth')\n",
    "        \n",
    "print('Total Training time:', end_time-start_time)\n",
    "print('No. of classes: {}, No. of support samples per class: {}, No. of query samples per class: {}'.format(\n",
    "                k, n, q))\n",
    "print('Epochs: {}, No. of batches in train: {}, No. of batches in val: {}, Each batch size: {}'.format(\n",
    "                epochs, total_train_batches, total_val_batches, batch_size))\n",
    "print('Training completed...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_test_batches = test_batch_loader.__len__()\n",
    "episode.test_file = open(logdir + '/test_log_r1',\"w\")\n",
    "per = episode.run_test_epoch(total_test_batches)\n",
    "\n",
    "print(\"Per of network : \", per*100)\n",
    "episode.test_file.write('\\nper = %f\\n'%(per))\n",
    "episode.test_file.close()\n",
    "print('Testing completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(loss,label='training')\n",
    "plt.plot(val_loss,label='validation')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Evolution of the loss function\")\n",
    "plt.legend()\n",
    "plt.savefig(logdir + \"/CTC_loss_r2.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(acc, label='training')\n",
    "plt.plot(val_acc,label='validation')\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Evolution of the WER function\")\n",
    "plt.legend()\n",
    "plt.savefig( logdir+ \"/error_r2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
